---
title: "Text Mining Project - M2 Data Mining"
author: "FOSSIER Camille - NEBBACHE Nabih"
date: "20/12/2019"
output: html_document
---

# Objectifs du projet et prise de notes

Eventuellement utiliser spacy

Selectionner les entités les plus présentes

Nettoyer : Si on voit que ce qui a été extrait est pas fou

Dans les meta information y'a des noms de personnes et des thématiques

Pareil, on peut faire du nettoyage dans les thématiques

L'étiquetage de topics peut etre fait à la main

En gros on veut une version enrichie du dataset

On a la date, le texte, mais on va rajouter des informations :

	- de topic
	- de personnes
	- de lieux
	- etc.

Il faut rajouter une granularité temporelle : anaalyse par mois ? par jour (bof) ?

Si le corpus est gros on peut choisir de s'intéresser à une période particulière

L'idée c'est de fournir un moteur de recherche un peu cool : moi ce qui m'intéresse c'est telle personne, etc

On aimerait bien avoir une visu :

	- par exemple un simple tableau
	- des graphes : qui relierait des personnes et des lieux par exemple, des topics, etc.
	- il faut avoir des petites idées
	- on peut imaginer une visu avant un événement, et après un événement
	- FAUT PRENDRE EN COMPTE LA DATE POUR FAIRE UNE ANALYSE
	- Ex: analyser comment un topic évolue dans le temps
	- faut rendre le truc un peu intéressant.
	- on peut le faire en notebook, pas forcément de rapport

```{r}
corpus = read.csv("../nyt.csv", sep="\t", stringsAsFactors=F)
corpus = corpus
colnames(corpus)
```

Detecting named entities

```{r, eval=FALSE}
#devtools::install_github("quanteda/spacyr", build_vignettes = FALSE)
library(spacyr)
#spacy_install(envname='anaconda3')
spacy_initialize(model = "en_core_web_sm", refresh_settings=TRUE)
```

Parsing entities and saving them

```{r, eval=FALSE}
parse_texts = list()

n = length(corpus$texts)

for (i in 1:n){
  text = corpus$texts[i]
  tmp = spacy_parse(text, entity = TRUE, nounphrase = TRUE)
  if (nrow(tmp) > 0) tmp$doc_id = paste(c("text", i), collapse="")
  parse_texts = rbind(parse_texts, tmp)
  print(paste(i,n, collapse = " / "))
}
entities = entity_extract(parse_texts, type = "all")
full$doc_id = as.numeric(gsub("text", "", full$doc_id))

write.csv(full, file="./data/full_entities.csv", row.names = F)
```

# Topic Modelling

Constructing vocabulary

```{r}
library(text2vec)
#library(stopwords)
library(tm)
library(dplyr)

# TODO : Clean quotes, etc. : " ' `
# But maybe above

iterator <- itoken(corpus$texts,
                   preprocessor=tolower, # replace capital letters
                   tokenizer=word_tokenizer, # split the text into unigrams
                   progressbar=F)

stop <- stopwords("english")
stop <- c(stop, "mr", "said")
vocabulary <- create_vocabulary(iterator, stopwords = stop)
vocabulary = vocabulary %>% prune_vocabulary(term_count_min = 10)
vectorizer <- vocab_vectorizer(vocabulary)
dtm = create_dtm(iterator, vectorizer)

```

Extracting topics

```{r}
lda_model = LDA$new(n_topics = 4,
                    doc_topic_prior = 0.1,
                    topic_word_prior = 0.01)

doc_topic_distr = lda_model$fit_transform(x = dtm,
                                          n_iter = 1000,
                                          convergence_tol = 0.001,
                                          n_check_convergence = 20,
                                          progressbar = F)
```

Trying to put names on topic

```{r}
# Most relevant words for each topic
lda_model$get_top_words(n = 15, lambda = 0.4)

# Ploting topics
lda_model$plot()

# Finding most relevant articles for each topic
ordered_articles = apply(doc_topic_distr, 2, function(t) order(t, decreasing = T))
titles = apply(ordered_articles[1:10,], 2, function(a) corpus[a,]$titles)
titles
```

THE FOLLOWING SHOULD MAYBE GO IN ANOTHER SCRIPT

Finding the 20 persons that appears most often

```{r, eval=FALSE}
entities = read.csv("./data/full_entities.csv")
persons = entities[entities$entity_type == "PERSON",]
unique_persons = unique(persons$entity)
persons_distrib = sapply(unique_persons, function(x) sum(persons$entity == x))

nb_to_keep = 20
persons_distrib = sort(persons_distrib, decreasing=T)[1:nb_to_keep]
```

Building the co-occurence matrix

```{r, eval=FALSE}
# Find in which docs each name appears
persons.docs <- lapply(names(persons_distrib),
                       function(x) persons$doc_id[which(persons$token==x)])

coocc.entities <- matrix(nrow=length(persons_distrib),
                         ncol=length(persons_distrib))

# Count names that appears in the same docs
for (i in 1:(length(persons_distrib)-1)) {
  coocc.entities[i,i] <- 0
  for (j in (i+1):length(persons_distrib)) {
     coocc.entities[i,j] <- length(intersect(persons.docs[[i]], persons.docs[[j]]))
     coocc.entities[j,i] <- coocc.entities[i,j]
  }
}

library(qgraph)

qgraph(coocc.entities,
       layout="spring",
       vsize=6,
       labels=names(persons_distrib),
       arrows=F)

```

```{r, eval=FALSE}
# TODO :

  # Phase d'enrichissement des données :

# Reconnaitre les entités nommées pour chaque article ?
# Afin d'etre en mesure de regrouper les articles par personnes
# Notebook 2 : Entités nommées

# Partie un peu illustrative pour le notebook :
# Faire des liens entre les entités les plus fréquentes avec un graphe (qgraph)

# Extraire les thématiques des articles
# Eventuellement faire du nettoyage ou de l'etiquetage à la main

# Sauvegarder les données enrichies pour nes pas avoir
# à refaire cette étape d'extraction

  # Phase d'exploitation des données enrichies

# Proposer un moteur de recherche pour trouver des articles avec
# plusieurs critères

# Proposer des fonctions pour afficher les résultats des recherches.

  # Structure du programme

# On travaille en notebook

# Un notebook enrichissement : Celui-ci

# Un notebook Moteur de recherche + Graphical Display

# Un notebook de test qui utilise justement le moteur de recherche

```

