---
title: "Text Mining Project - M2 Data Mining"
author: "FOSSIER Camille - NEBBACHE Nabih"
date: "20/12/2019"
output: html_document
---

# Visualisation du corpus

Ce document a pour objectif de visualiser un peu mieux les données du corpus en
utilisant les données du corpus enrichi et la modélisation des thématiques.

# Axes de visualisation

## Evolution des thématiques dans le temps

```{r}
topics = read.csv("./data/topics_probabilities.csv")
corpus = read.csv("./data/nyt.csv", sep="\t", stringsAsFactors=F)
dates = as.POSIXlt(corpus$dates)
dates$mday = rep(1, length(dates))
dates$mon = unlist(lapply(dates$mon, function(m) floor(m/4) * 4))

topics_d = topics
topics_d$dates = unlist(lapply(dates, as.String))

library(dplyr)
amounts = topics_d %>% 
  group_by(dates) %>%
  summarise(t1 = sum(V1),
            t2 = sum(V2),
            t3 = sum(V3),
            t4 = sum(V4))
amounts$num_dates = as.POSIXlt(amounts$dates)

plot(amounts$num_dates, amounts$t1, type="l", col="red")
lines(amounts$num_dates, amounts$t2, col="green")
lines(amounts$num_dates, amounts$t3, col="blue")
lines(amounts$num_dates, amounts$t4)

```

## Liens entre certaines entités

Keeping only persons and sorting them by appearance

```{r}
entities = read.csv("./data/full_entities.csv")
persons = entities[entities$entity_type == "PERSON",]

full_persons_distrib = persons %>% 
  group_by(entity) %>%
  summarise(distrib = length(entity))

full_persons_distrib = full_persons_distrib[order(full_persons_distrib$distrib, decreasing = T),]
```

Removing doubles and keeping the 20 most frequent

```{r}
nb_to_keep = 20

full_persons_distrib = full_persons_distrib[1:(nb_to_keep * nb_to_keep),]
n = nrow(full_persons_distrib)

# For every pair, if one is a substring of the other
# then we keep the most generic
# Example : "Gorbatchev" and "Mikhail_Gorbatchev" => Only "Gorbatchev"
for (i in 1:(n - 1)) {
  fi = full_persons_distrib[i,]
  for (j in (i+1):n) {
    fj = full_persons_distrib[j,]
    if (grepl(fi$entity, fj$entity, fixed = T)) {
      full_persons_distrib[i,]$distrib = fi$distrib + fj$distrib
      full_persons_distrib[j,]$distrib = 0
    }
    else if (grepl(fj$entity, fi$entity, fixed = T)) {
      full_persons_distrib[j,]$distrib = fi$distrib + fj$distrib
      full_persons_distrib[i,]$distrib = 0
    }
  }
}

full_persons_distrib = full_persons_distrib[order(full_persons_distrib$distrib, decreasing = T),]
persons_distrib = full_persons_distrib[1:nb_to_keep,]

full_persons_distrib
```

Building the co-occurence matrix

```{r}
# Find in which docs each name appears
persons.docs <- lapply(persons_distrib$entity,
                       function(x) persons$doc_id[which(persons$entity==x)])

coocc.entities <- matrix(nrow=nrow(persons_distrib),
                         ncol=nrow(persons_distrib))

# Count names that appears in the same docs
for (i in 1:(nrow(persons_distrib)-1)) {
  coocc.entities[i,i] <- 0
  for (j in (i+1):nrow(persons_distrib)) {
     coocc.entities[i,j] <- length(intersect(persons.docs[[i]], persons.docs[[j]]))
     coocc.entities[j,i] <- coocc.entities[i,j]
  }
}

library(qgraph)

qgraph(coocc.entities,
       layout="spring",
       vsize=6,
       labels=persons_distrib$entity,
       arrows=F)

```