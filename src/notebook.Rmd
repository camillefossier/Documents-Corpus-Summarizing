---
title: "Text Mining Project - M2 Data Mining"
author: "FOSSIER Camille - NEBBACHE Nabih"
date: "20/12/2019"
output: html_document
---

# Objectifs du projet et prise de notes

Eventuellement utiliser spacy

Selectionner les entités les plus présentes

Nettoyer : Si on voit que ce qui a été extrait est pas fou

Dans les meta information y'a des noms de personnes et des thématiques

Pareil, on peut faire du nettoyage dans les thématiques

L'étiquetage de topics peut etre fait à la main

En gros on veut une version enrichie du dataset

On a la date, le texte, mais on va rajouter des informations :

	- de topic
	- de personnes
	- de lieux
	- etc.

Il faut rajouter une granularité temporelle : anaalyse par mois ? par jour (bof) ?

Si le corpus est gros on peut choisir de s'intéresser à une période particulière

L'idée c'est de fournir un moteur de recherche un peu cool : moi ce qui m'intéresse c'est telle personne, etc

On aimerait bien avoir une visu :

	- par exemple un simple tableau
	- des graphes : qui relierait des personnes et des lieux par exemple, des topics, etc.
	- il faut avoir des petites idées
	- on peut imaginer une visu avant un événement, et après un événement
	- FAUT PRENDRE EN COMPTE LA DATE POUR FAIRE UNE ANALYSE
	- Ex: analyser comment un topic évolue dans le temps
	- faut rendre le truc un peu intéressant.
	- on peut le faire en notebook, pas forcément de rapport
	
# Importation des librairies 
```{r}
library(Matrix)
library(sets)
library(spacyr)
library(text2vec)
#library(stopwords)
library(tm)
library(stringr)
library(dplyr)

source("functions.R")
```


# Extraction du corpus et prétraitement sur ce dernier

## Extraction du corpus

```{r}
corpus = read.csv(corpus_path, sep="\t", stringsAsFactors=F)
colnames(corpus)
```
## Nettoyage des données
On commence par enlever les colones qui n'ont pas de texte
```{r}

#On enleve les texte dont le nomber de caractère est égal à 0
has_text = corpus[str_trim(corpus$texts) != "",]

#Un petit récapitulatif sur la distribution du number de mots dans notre corpus
l = unlist(lapply(has_text$texts, count_words))
summary(l)
hist(l, length(l))
```
On remarque que la longueur minimale d'un texte est de 2. Analysons ces textes
```{r}
shorts = has_text[l <=25,]
sl = unlist(lapply(shorts$texts, count_words))
summary(sl)
```
Au dessous de la longeur 25, la taille maximale (par mot) des articles est de 4. Ces articles ne paraissent pas important pour une tache commme la recherche de documents.
Et on analysant un peu plus, on voit (dans la variable related articles) que  les sujets précedents et suivants n'ont pas de relation.  
```{r}
ind = which(l == 3)[3]
related_articles = has_text[(ind-2):(ind+2)  ,]

```
Nous allons donc les supprimer.
```{r}
has_min_text = has_text[l >= 25,]
```
Voyons ce que les autres colones nous dévoilent
```{r}
title1 = has_min_text$principal_classifier
title1[1:10]
```

Les première valeurs sont similaire pour la colone "principal_classifier". Voyons combien y a-t-il de valeurs distinctes
```{r}
dist_title1 = unique(title1)
length(dist_title1)
```

Une seule valeur dans cette colone, les articles ont surement été filtrés au préalable ;). On la supprime alors.
```{r}
has_min_text1 = has_min_text[, -4]
colnames(has_min_text1)
```
Pour les auters colones ?
```{r}
has_min_text1$titles[1:10]
has_min_text1[1:200, 4:5]
```

La colone titre parrait interessante. Voyons si on peut enrichire notre texte avec. Nous allons compter le taux de présence de mots de chaque titre par rapport a son texte.  
```{r}
rate <- function(x,y){
  require(sets)
  x = tolower(x)
  y = tolower(y)
  wx <- unlist(word_tokenizer(x))
  wy <- unlist(word_tokenizer(y))
  ly = length(wy)
  inter = intersect(wx,wy)
  li = length(inter)
  li/ly
}
#test = has_min_text1[1:4,]
#apply(test,1, function(line) paste(line[[3]], line[[4]], sep =" "))

intersection = apply(has_min_text1, 1, function(line) rate(line[[2]], line[[3]]))
summary(intersection)
nas_idx = is.na(intersection)
```





En moyenne, 0.63 des mots présent apparaissent dans le texte, l'autre tier pourrait alors enrichir le texte. Voyons voir ce que sont à peu près ces mots. 
```{r}
diff <- function(x,y){
  require(sets)
  x = tolower(x)
  y = tolower(y)
  wx <- unlist(word_tokenizer(x))
  wy <- unlist(word_tokenizer(y))
  setdiff(wy, wx)
}
title_only_words = unlist(apply(has_min_text1, 1, function(line) diff(line[[2]], line[[3]])))
word_count = table(title_only_words)
sorted_word_count =sort(word_count,decreasing = T)
sorted_word_count[1:40]

```

Beaucoup de ces mots sont des "stop words". Mais beaucoup aussi sont des mots très significatifs, et puisque nous allons traiter les "stop words" dans la suite, ajouter le titre au texte sera donc bénéfique.
```{r}
clean_corpus = has_min_text1
clean_corpus$texts = apply(clean_corpus,1, function(line) paste(line[[3]], line[[2]], sep =" ."))
clean_corpus = clean_corpus[,-3]
```

# Creation d'un vocabulaire et nettoyage par mot

## Creation du vocabulaire
```{r}
iterator = get_iterator(clean_corpus$texts)
vocabulary = get_vocabulary(iterator)
vocabulary = vocabulary[vocabulary$term_count >5,]
dim(vocabulary)
```
On analyse les mots qui ne sont pas completement alphabetiques
```{r}
words = vocabulary$term
non_alphabetics = words[!words %>% str_detect(regex("^[[a-z]|[A-Z]]+$"))]

non_alphabetics[1:100]
```
On enleve les chiffres
```{r}
num_regex = "^\\d+(,\\d+)*(\\.\\d+)?$"
non_alph_num= non_alphabetics[!non_alphabetics %>% str_detect(regex(num_regex))]
non_alph_num[1:100]
```
on remarque qu'il ya beacoup de mot du genre ****'s. Enlevons les pour voir ce qui reste
```{r}
poss_regex = "^[[a-z][A-Z]]+'s$"
non_alph_num_poss= non_alph_num[!non_alph_num %>% str_detect(regex(poss_regex))]
non_alph_num_poss
```
Ce qui restent sont soit des initiale ou des mot de classement. 
Le traitement peut se faire sur les mots qui sont sous la forme *****'s, en séparant le "'s" pour récuperer le mot.
On eleve les mots ayant une taille de 1 (On pourrait faire plus, mais on risque de perdre de l'information)
```{r}

poss_regex2 = "^([[a-z][A-Z]']+)'s$"

final_cleaning  <- function(words){
  res = words %>% str_replace(poss_regex2, "\\1")
  res[nchar(res) > 1] 
}


chain = clean_corpus$texts %>% tolower %>% word_tokenizer
achain = lapply(chain, final_cleaning)
it =  itoken(achain)
clean_vocab = get_vocabulary(it)
clean_vocab = clean_vocab[clean_vocab$term_count >5,]

### regroupement des mots similaires
```
# Creation de la matrice terme document (en utilisant tfidf) et sauvegarde
```{r}

dtm = get_dtm(clean_vocab,it,T)
writeMM(obj = dtm, file = dtm_path)
write.csv2(data.frame(colnames(dtm)),dtm_colnames_path, row.names = T)


```




#Detection des entitées nommées

Dans cette partie, on extrait les entités nommées du text. On utilise la librairie Spacy pour cela.
```{r, eval=FALSE}
#devtools::install_github("quanteda/spacyr", build_vignettes = FALSE)
#spacy_install(envname='anaconda3')
spacy_initialize(model = "en_core_web_sm", refresh_settings=TRUE)
```

On extrait les entités sur le corpus original, car ayant essayé sur le corpus traité, les résultats sont moins bon.
```{r, eval=FALSE}

parsed_texts= spacy_parse(corpus$text, entity = TRUE, nounphrase = TRUE)
entities = entity_extract(parsed_texts, type = "all")
entities$doc_id = as.numeric(gsub("text", "", entities$doc_id))
write.csv(entities, file=entities_path, row.names = F)

```
On créer à partir de ces entités nommées les matrices qui montrent les relations : 
- personne * document
- lieu * document
- evenement *document

Ces matrices servent à faire la recherche par entitée.

```{r}
matrices = get_doc_matrices(entities,T)
for(mat_name in names(matrices)){
  mat_path = paste(entity_matrices_root_path,mat_name,".mtx", sep = "")
  mat_names_path = paste(entity_matrices_root_path, mat_name, ".csv", sep="")
  mat = matrices[[mat_name]]
  writeMM(obj = mat, file = mat_path)
  write.csv2(data.frame(rownames(mat)),mat_names_path, row.names = T)

}
# save those matrices TODO

```

# Topic Modelling
Cette partie consiste à affecter à chaque document une probabilité d'appartenir à différents topics. Ces derniers sont composés de plusieurs mots les définissants.


```{r}
lda_model = LDA$new(n_topics = 4,
                    doc_topic_prior = 0.1,
                    topic_word_prior = 0.01)

doc_topic_distr = lda_model$fit_transform(x = dtm,
                                          n_iter = 1000,
                                          convergence_tol = 0.001,
                                          n_check_convergence = 20,
                                          progressbar = F)

write.csv(doc_topic_distr, file=topics_path, row.names = F)
```

Trying to put names on topic

```{r}
# Most relevant words for each topic
lda_model$get_top_words(n = 15, lambda = 0.4)

# Ploting topics
lda_model$plot()
```

THE FOLLOWING SHOULD MAYBE GO IN ANOTHER SCRIPT

Finding the 20 persons that appears most often

```{r, eval=FALSE}
entities = read.csv(entities_path)
persons = entities[entities$entity_type == "PERSON",]
unique_persons = unique(persons$entity)
persons_distrib = sapply(unique_persons, function(x) sum(persons$entity == x))

nb_to_keep = 20
persons_distrib = sort(persons_distrib, decreasing=T)[1:nb_to_keep]
```

Building the co-occurence matrix

```{r, eval=FALSE}
# Find in which docs each name appears
persons.docs <- lapply(names(persons_distrib),
                       function(x) persons$doc_id[which(persons$token==x)])

coocc.entities <- matrix(nrow=length(persons_distrib),
                         ncol=length(persons_distrib))

# Count names that appears in the same docs
for (i in 1:(length(persons_distrib)-1)) {
  coocc.entities[i,i] <- 0
  for (j in (i+1):length(persons_distrib)) {
     coocc.entities[i,j] <- length(intersect(persons.docs[[i]], persons.docs[[j]]))
     coocc.entities[j,i] <- coocc.entities[i,j]
  }
}

library(qgraph)

qgraph(coocc.entities,
       layout="spring",
       vsize=6,
       labels=names(persons_distrib),
       arrows=F)

```

```{r, eval=FALSE}
# TODO :

  # Phase d'enrichissement des données :

# Reconnaitre les entités nommées pour chaque article ?
# Afin d'etre en mesure de regrouper les articles par personnes
# Notebook 2 : Entités nommées

# Partie un peu illustrative pour le notebook :
# Faire des liens entre les entités les plus fréquentes avec un graphe (qgraph)

# Extraire les thématiques des articles
# Eventuellement faire du nettoyage ou de l'etiquetage à la main

# Sauvegarder les données enrichies pour nes pas avoir
# à refaire cette étape d'extraction

  # Phase d'exploitation des données enrichies

# Proposer un moteur de recherche pour trouver des articles avec
# plusieurs critères

# Proposer des fonctions pour afficher les résultats des recherches.

  # Structure du programme

# On travaille en notebook

# Un notebook enrichissement : Celui-ci

# Un notebook Moteur de recherche + Graphical Display

# Un notebook de test qui utilise justement le moteur de recherche

```

